{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Learning"
      ],
      "metadata": {
        "id": "LCiL4Q-IPIkE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1.** What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        "\n",
        "- Ensemble Learning in machine learning is a technique where multiple models (often called \"weak learners\") are combined to create a stronger, more accurate, and more robust predictive model.\n",
        "Instead of relying on a single model, ensemble learning leverages the strengths of many models to reduce errors and improve generalization.\n",
        "\n",
        "- Key Idea Behind Ensemble Learning\n",
        "\n",
        " The central idea is:\n",
        "“A group of weak models, when combined properly, can perform better than any single strong model.”\n",
        "\n",
        " This works because:\n",
        "\n",
        "1. Error Reduction – Different models may make different mistakes; combining them cancels out individual errors.\n",
        "\n",
        "2. Variance Reduction – Some models (like decision trees) are highly sensitive to small data changes. Using an ensemble averages out these fluctuations.\n",
        "\n",
        "3. Bias Reduction – Combining diverse models can reduce the bias of individual learners.\n",
        "\n",
        "**Q2.** What is the difference between Bagging and Boosting?\n",
        "\n",
        " - Bagging (Bootstrap Aggregating)\n",
        "\n",
        "1. Trains models independently and in parallel.\n",
        "\n",
        "2. Uses bootstrap sampling (random sampling with replacement).\n",
        "\n",
        "3. Combines predictions using majority voting (classification) or averaging (regression).\n",
        "\n",
        "4. Focuses on reducing variance (good for overfitting models).\n",
        "\n",
        "5. Works best with high variance, low bias models like decision trees.\n",
        "\n",
        "6. Less prone to overfitting.\n",
        "\n",
        "7. Example: Random Forest.\n",
        "\n",
        "- Boosting\n",
        "\n",
        "1. Trains models sequentially, each new model learns from the errors of the previous ones.\n",
        "\n",
        "2. Uses the entire dataset, but gives higher weight to misclassified samples.\n",
        "\n",
        "3. Combines predictions using a weighted sum of models.\n",
        "\n",
        "4. Focuses on reducing bias (good for underfitting models).\n",
        "\n",
        "5. Works best with weak learners like shallow decision stumps.\n",
        "\n",
        "6. More prone to overfitting if not tuned properly.\n",
        "\n",
        "7. Examples: AdaBoost, Gradient Boosting, XGBoost, LightGBM.\n",
        "\n",
        "**Q3.** What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?\n",
        "\n",
        "- Bootstrap sampling is a statistical method where we create new datasets by randomly selecting data points **with replacement** from the original dataset. This means the same data point can appear multiple times in one sample, while some points may be left out. In Bagging methods like Random Forest, bootstrap sampling ensures that each model (tree) is trained on a slightly different dataset, creating diversity among models, reducing variance, and improving overall prediction accuracy.\n",
        "\n",
        "- Role in Bagging (e.g., Random Forest)\n",
        "\n",
        "1. Bootstrap Sampling Creates Diversity\n",
        "\n",
        " - Each model (tree) is trained on a different bootstrap sample of the data.\n",
        "\n",
        " - This ensures that not all models see the same data, reducing correlation among them.\n",
        "\n",
        "2. Reduces Variance\n",
        "\n",
        " - Since trees are trained on slightly different data, their predictions vary.\n",
        "\n",
        " - Combining them (by averaging/voting) cancels out individual errors → lowers variance.\n",
        "\n",
        "3. Improves Generalization\n",
        "\n",
        " - Models are less likely to overfit to the training data, since each tree only sees part of it.\n",
        "\n",
        " - The \"wisdom of crowds\" effect makes the final prediction more robust.\n",
        "\n",
        "4. Out-of-Bag (OOB) Estimation\n",
        "\n",
        " - The ~37% of data not included in a bootstrap sample (called Out-of-Bag data) can be used as a built-in validation set to estimate accuracy without a separate test set.\n",
        "\n",
        "**Q4.** What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "\n",
        " - Out-of-Bag (OOB) Samples\n",
        "\n",
        "  - In bootstrap sampling, each model (e.g., decision tree in Random Forest) is trained on a random sample of the data with replacement.\n",
        "\n",
        "  - On average, about 63% of the original dataset is included in the bootstrap sample for training a tree.\n",
        "\n",
        "  - The remaining ~37% of data that is not chosen for that tree is called the Out-of-Bag (OOB) samples.\n",
        "\n",
        " - How OOB Score is Used:\n",
        "\n",
        "  - After a tree is trained, it can be tested on its OOB samples (since those data points were not used in training).\n",
        "\n",
        "  -  By aggregating predictions from all trees for which a sample was OOB, we can evaluate the model’s accuracy.\n",
        "\n",
        "  -  This accuracy is called the OOB Score.  \n",
        "\n",
        "**Q5.** Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.  \n",
        "\n",
        "- Feature Importance in a Single Decision Tree\n",
        "\n",
        "1. How it’s calculated:\n",
        "\n",
        "  - At each split, the tree chooses the feature that provides the highest information gain (e.g., Gini reduction, entropy reduction, variance reduction).\n",
        "\n",
        "  - Feature importance = sum of the improvements (impurity reduction) that feature provides across all splits, normalized to 1.\n",
        "\n",
        "2. Characteristics:\n",
        "\n",
        "  - Importance depends heavily on how the tree is structured.\n",
        "\n",
        "  - A feature used near the top of the tree usually appears more important.\n",
        "\n",
        "   - Can be unstable: small changes in data can change the tree structure → importance scores shift.\n",
        "\n",
        "- Feature Importance in a Random Forest\n",
        "\n",
        "1. How it’s calculated:\n",
        "\n",
        "  - Each tree in the forest computes its own feature importance (same method as above).\n",
        "\n",
        "  - The final importance = average of all trees’ importances.\n",
        "\n",
        "2. Characteristics:\n",
        "\n",
        "  - More stable and reliable than a single tree because results are averaged across many trees.\n",
        "\n",
        "  - Less biased toward features that dominate a single tree.\n",
        "\n",
        "  - Provides a better global view of which features consistently matter for prediction.   \n",
        "\n",
        "- Comparison (Tree vs. Forest)\n",
        "\n",
        "1. Decision Tree:\n",
        "\n",
        "  - Importance = impurity reduction in that single tree.\n",
        "\n",
        "  - Can be unstable, biased by tree depth/structure.\n",
        "\n",
        "2. Random Forest:\n",
        "\n",
        "  - Importance = averaged impurity reductions across many trees.\n",
        "\n",
        "  - More robust, less sensitive to noise, more reliable measure.  "
      ],
      "metadata": {
        "id": "UpqpR6fqPMeN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3Jhvu6uGOxUh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3broDh3FOf4z",
        "outputId": "d977c528-4c57-46bd-b099-43875492a8ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ],
      "source": [
        "#Q6. Write a Python program to:\n",
        "# ● Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer()\n",
        "# ● Train a Random Forest Classifier\n",
        "# ● Print the top 5 most important features based on feature importance scores.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "importances = clf.feature_importances_\n",
        "\n",
        "feat_importances = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "feat_importances = feat_importances.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(feat_importances.head(5))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q7. Write a Python program to:\n",
        "# ● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "# ● Evaluate its accuracy and compare with a single Decision Tree\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import sklearn\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "dt_acc = accuracy_score(y_test, dt_pred)\n",
        "\n",
        "if sklearn.__version__ >= \"1.2\":\n",
        "    bagging = BaggingClassifier(\n",
        "        estimator=DecisionTreeClassifier(random_state=42),\n",
        "        n_estimators=50,\n",
        "        random_state=42\n",
        "    )\n",
        "else:\n",
        "    bagging = BaggingClassifier(\n",
        "        base_estimator=DecisionTreeClassifier(random_state=42),\n",
        "        n_estimators=50,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "bagging.fit(X_train, y_train)\n",
        "bagging_pred = bagging.predict(X_test)\n",
        "bagging_acc = accuracy_score(y_test, bagging_pred)\n",
        "\n",
        "print(\"Accuracy of Single Decision Tree:\", dt_acc)\n",
        "print(\"Accuracy of Bagging Classifier with Decision Trees:\", bagging_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20QyCrlgcveT",
        "outputId": "0517e16c-b3d4-4af3-fa08-5829f9784663"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Single Decision Tree: 1.0\n",
            "Accuracy of Bagging Classifier with Decision Trees: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q8. Write a Python program to:\n",
        "# ● Train a Random Forest Classifier\n",
        "# ● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "# ● Print the best parameters and final accuracy\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 5, 10, 20]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "y_pred = best_rf.predict(X_test)\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Final Test Accuracy:\", acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hX2Y_IEYdMge",
        "outputId": "ade84e69-5a35-45de-e502-c40196426b7b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 200}\n",
            "Final Test Accuracy: 0.9707602339181286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q9. Write a Python program to:\n",
        "# ● Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
        "# ● Compare their Mean Squared Errors (MSE)\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "bagging_reg = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(random_state=42),\n",
        "    n_estimators=50,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "bagging_pred = bagging_reg.predict(X_test)\n",
        "bagging_mse = mean_squared_error(y_test, bagging_pred)\n",
        "\n",
        "rf_reg = RandomForestRegressor(\n",
        "    n_estimators=50,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_reg.fit(X_train, y_train)\n",
        "rf_pred = rf_reg.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "print(\"Mean Squared Error - Bagging Regressor:\", bagging_mse)\n",
        "print(\"Mean Squared Error - Random Forest Regressor:\", rf_mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "om47M5xkfcVq",
        "outputId": "045df423-464e-4cd3-a17a-296e61943a76"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error - Bagging Regressor: 0.25787382250585034\n",
            "Mean Squared Error - Random Forest Regressor: 0.25772464361712627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10.** You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data.\n",
        "\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "\n",
        "Explain your step-by-step approach to:\n",
        "\n",
        "● Choose between Bagging or Boosting\n",
        "\n",
        "● Handle overfitting\n",
        "\n",
        "● Select base models\n",
        "\n",
        "● Evaluate performance using cross-validation\n",
        "\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "\n",
        "\n",
        " - If I were working on predicting loan defaults, I’d approach it like this:\n",
        "\n",
        "**Step 1: Choosing Bagging vs Boosting**\n",
        "Since missing a defaulter is very costly for a bank, I’d lean towards Boosting methods (like XGBoost or LightGBM). Boosting focuses more on the difficult-to-predict customers and reduces bias, which is important here. Bagging is great for reducing variance, but in this case, catching those hard-to-detect defaults is more critical.\n",
        "\n",
        "**Step 2: Handling Overfitting**\n",
        "Boosting models can overfit if they grow too complex. To control this, I’d tune parameters like the learning rate, maximum tree depth, and number of estimators. I’d also use techniques like early stopping and feature selection to make sure the model doesn’t just memorize the training data.\n",
        "\n",
        "**Step 3: Selecting Base Models**\n",
        "For Bagging or Boosting, the usual base model is a decision tree. For Boosting, I’d use shallow decision trees (sometimes called stumps) because they’re weak learners that work well when combined. If I explore Stacking, I’d consider mixing logistic regression (for linear patterns) with tree-based models (for nonlinear patterns).\n",
        "\n",
        "**Step 4: Evaluating Performance**\n",
        "Since defaults are usually rare compared to non-defaults, I’d use stratified k-fold cross-validation to keep the class balance. Accuracy alone isn’t enough here, so I’d look at AUC-ROC, precision-recall, and F1-score to properly judge performance.\n",
        "\n",
        "**Step 5: Why Ensemble Learning Helps**\n",
        "Using ensembles gives more reliable predictions. Boosting in particular makes the model focus on tough cases and improves detection of defaulters. Overall, it reduces both bias and variance, which means the bank gets a model that generalizes better. In real terms, this leads to fewer risky loans slipping through, smarter lending decisions, and reduced financial loss.\n"
      ],
      "metadata": {
        "id": "cGEe_X_JgySl"
      }
    }
  ]
}